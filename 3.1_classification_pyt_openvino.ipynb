{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchの画像分類モデルを変換してOpenVINOで使う方法を学ぶ  \n",
    "\n",
    "このチュートリアルではPyTorchの画像分類モデルをOpenVINO IRモデルに変換する方法について学びます。  \n",
    "\n",
    "途中でPyTorchで推論をした場合と、OpenVINOで推論をした場合の簡易パフォーマンス比較も行います。OpenVINOにより推論が高速化されることを確認してください。\n",
    "\n",
    "また、後半では推論プログラムの効率を向上させるカギとなる、非同期推論APIのサンプルコードを見ながら、基本的な使い方を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import openvino as ov\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchの学習済みモデルの読み込み (ダウンロード)  \n",
    "\n",
    "学習済みresnet50モデルをtorch hubからダウンロードします。  \n",
    "`.eval()` APIを呼び出し、モデルを推論モードにしておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT, progress=True)\n",
    "pt_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchモデルをOpenVINO IRモデルに変換し、ファイルとして保存する  \n",
    "\n",
    "`convert_model()` APIを使用することでモデルを変換することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ov.convert_model(pt_model)\n",
    "ov.save_model(ov_model, 'resnet50.xml', compress_to_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存したOpenVINO IRモデルファイルを読み込み、デバイス向けにコンパイルする  \n",
    "\n",
    "読み込んだモデルは入力画像サイズ(H, W)やバッチサイズ(N)が不定(?)のダイナミックシェイプモデルになっています。`reshape()` APIでシェイプを指定し、スタティックシェイプモデルにしています。この処理は行わなくてもモデルの実行は可能です (ダイナミックシェイプのまま実行可能)。ですが、スタティックシェイプにすることでほんの少しパフォーマンスが向上し、メモリ使用量も削減されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ov.Core().read_model('resnet50.xml')\n",
    "print(ov_model.inputs[0])\n",
    "ov_model.reshape((1,3,224,224))                   # 読み込んだモデルのシェイプは[?,3,?,?]のダイナミックシェイプモデルなので、(1,3,224,224)にリシェイプしてスタティックシェイプモデルにする (オプショナル)\n",
    "print(ov_model.inputs[0])\n",
    "compiled_model = ov.compile_model(ov_model, device_name='GPU.0', config={'CACHE_DIR':'./cache'})\n",
    "compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力画像の読み込みとプリプロセス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('beaver.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (224,224))\n",
    "display(Image.fromarray(img))\n",
    "img = img.astype(np.float32) / 255.0 - 0.5\n",
    "img = np.transpose(img, (2,0,1))\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINOで推論実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled_model.infer_new_request(img)\n",
    "result = result[0].ravel()\n",
    "print(result.shape, result[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論結果の表示  \n",
    "\n",
    "正しい推論結果が出ていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synset_words.txt') as f:\n",
    "    imagenet_labels = [ line.rstrip().split(maxsplit=1)[1] for line in f.readlines() ]\n",
    "\n",
    "indices = np.argsort(result)[::-1]\n",
    "\n",
    "for index in indices[:5]:\n",
    "    print(index, imagenet_labels[index], result[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchでの推論時間を計測  \n",
    "\n",
    "同じ入力データを使用してPyTorch (CPU)で推論を実行し時間計測を行います。  \n",
    "`%%timeit`はJupyter notebookの機能で、続くコマンドを複数回実行し、パフォーマンスデータを表示してくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "pt_tensor = torch.Tensor(img)\n",
    "\n",
    "%timeit pt_model(pt_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINOでの推論時間を計測  \n",
    "\n",
    "同様に、OpenVINOで推論を行った時の推論時間を計測します。PyTorchと実行時間を比較してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenVINO - Synchronous inference\n",
    "\n",
    "%timeit compiled_model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO非同期推論API その１  \n",
    "\n",
    "ここではコールバック関数を使用せず、非同期推論を行っています。  \n",
    "`AsyncInferQueue()`で推論要求キューを作成すると、自動でキューの管理を行ってくれるので便利です。これを使わずに`create_infer_request()` APIで推論要求バッファを作成した場合は要求バッファの空きや使用状況管理をユーザープログラム側で行わなくてはならなくなります。  \n",
    "最後に`async_queue.wait_all()`ですべての要求が処理されたことを確認しています。  \n",
    "\n",
    "このケースではプリプロセスやポストプロセスもなく、推論処理も軽いため同期推論と非同期推論のパフォーマンス差はほとんどありません。推論プログラムが複雑になり、プリ・ポストプロセスが重くなるにつれ、非同期推論の重要性は増してきますので覚えておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# OpenVINO - Asynchronous inference\n",
    "\n",
    "async_queue = ov.AsyncInferQueue(compiled_model, jobs=4)\n",
    "niter = 100\n",
    "\n",
    "for _ in range(niter):\n",
    "    #while async_queue.is_ready() == False: pass\n",
    "    async_queue.start_async(img)\n",
    "\n",
    "async_queue.wait_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO非同期推論API その２  \n",
    "\n",
    "この例では、推論要求バッファ(`infer_request`)を２つ作成し、交互に推論要求を投入するようにしています。  \n",
    "ほとんどの推論プログラムではこのような使用方法で十分プリ・ポストプロセス処理を隠ぺいすることが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "async_queue = [ compiled_model.create_infer_request() for _ in range(2) ]\n",
    "curr_id = 0\n",
    "niter = 100\n",
    "\n",
    "for _ in range(niter):\n",
    "    next_id = 1 - curr_id\n",
    "    async_queue[next_id].wait()\n",
    "    async_queue[next_id].start_async(img)\n",
    "\n",
    "for i in range(2):\n",
    "    async_queue[i].wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## OpenVINO非同期推論API その３  \n",
    "\n",
    "この例では推論結果をコールバック関数で受け取るようになっています。これにより推論結果のポーリング的処理が不要となり推論ループは簡潔になりますが、同時に完全な非同期処理が必要となり、全体のシーケンス管理がやや複雑になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "async_queue = [ compiled_model.create_infer_request() for _ in range(2) ]\n",
    "curr_id = 0\n",
    "niter = 100\n",
    "\n",
    "def infer_callback(user_data):\n",
    "    res, infer_id, postprocess = user_data\n",
    "    if postprocess:\n",
    "        # Post processing\n",
    "        res = res.output_tensors[0].data\n",
    "        class_id = np.argmax(res)\n",
    "        print(infer_id, class_id)\n",
    "\n",
    "for i in range(niter):\n",
    "    next_id = 1 - curr_id\n",
    "    async_queue[next_id].wait()\n",
    "    async_queue[next_id].set_callback(callback=infer_callback, userdata=[async_queue[next_id], i, False])\n",
    "    async_queue[next_id].start_async(img)\n",
    "\n",
    "for i in range(2):\n",
    "    async_queue[i].wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "ここでは簡単にPyTorchのモデルを`convert_model()` APIで変換し、OpenVINOで利用できることを学びました。  \n",
    "また、OpenVINOとPyTorchでの推論パフォーマンス比較も行いました。多くの場合、OpenVINOを利用することによって、インテルハードウエアの持つ機能を最大限活用することが可能となり、推論パフォーマンスが向上します。これにより、従来は外付けGPUなどが必要と思われていた処理がPCのCPUでも十分処理できるようになり、応用範囲が広がり、BOMコスト低減に貢献します。\n",
    "\n",
    "最後に非同期推論APIの利用例も見てきました。プログラムが複雑になり、モデルのプリ・ポスト処理が重くなるほど非同期推論の重要性は増してきます。  \n",
    "CPU以外の推論デバイスに推論処理をオフロードしたときも非同期推論は重要になります。せっかく推論処理をオフロードしても、推論処理が終わるまでCPUが待ちぼうけているのでは意味がありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
